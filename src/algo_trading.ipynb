{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intelligent Algorithmic Trading\n",
    "# Paper Title\n",
    "\n",
    "<hr>\n",
    "For further details visit our <a href='https://github.com/achmand/ari5123_assignment' target=\"_blank\">GitHub Repository</a>\n",
    "\n",
    "### Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import lucrum.algo as algo\n",
    "import lucrum.datareader as ldr\n",
    "from ipywidgets import interactive\n",
    "from collections import OrderedDict\n",
    "from lucrum.algo import MACrossoverAlgo\n",
    "\n",
    "# hide warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Cryptocurrencies Datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BTCUSDT file existed, retrieving data from file.\n",
      "open_time     datetime64[ns]\n",
      "open                 float64\n",
      "high                 float64\n",
      "low                  float64\n",
      "close                float64\n",
      "close_time    datetime64[ns]\n",
      "trades                 int64\n",
      "volume               float64\n",
      "dtype: object\n",
      "ETHUSDT file existed, retrieving data from file.\n",
      "open_time     datetime64[ns]\n",
      "open                 float64\n",
      "high                 float64\n",
      "low                  float64\n",
      "close                float64\n",
      "close_time    datetime64[ns]\n",
      "trades                 int64\n",
      "volume               float64\n",
      "dtype: object\n",
      "XRPUSDT file existed, retrieving data from file.\n",
      "open_time     datetime64[ns]\n",
      "open                 float64\n",
      "high                 float64\n",
      "low                  float64\n",
      "close                float64\n",
      "close_time    datetime64[ns]\n",
      "trades                 int64\n",
      "volume               float64\n",
      "dtype: object\n",
      "LTCUSDT file existed, retrieving data from file.\n",
      "open_time     datetime64[ns]\n",
      "open                 float64\n",
      "high                 float64\n",
      "low                  float64\n",
      "close                float64\n",
      "close_time    datetime64[ns]\n",
      "trades                 int64\n",
      "volume               float64\n",
      "dtype: object\n",
      "EOSUSDT file existed, retrieving data from file.\n",
      "open_time     datetime64[ns]\n",
      "open                 float64\n",
      "high                 float64\n",
      "low                  float64\n",
      "close                float64\n",
      "close_time    datetime64[ns]\n",
      "trades                 int64\n",
      "volume               float64\n",
      "dtype: object\n",
      "XLMUSDT file existed, retrieving data from file.\n",
      "open_time     datetime64[ns]\n",
      "open                 float64\n",
      "high                 float64\n",
      "low                  float64\n",
      "close                float64\n",
      "close_time    datetime64[ns]\n",
      "trades                 int64\n",
      "volume               float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# set parameters for importing datasets\n",
    "data_path = \"data/\"            # data is stored in data directory \n",
    "data_format = \".csv\"           # file is saved as .csv format\n",
    "data_source = \"binance\"        # Binance API will be used to download data\n",
    "data_interval = \"15m\"          # we will be fetching 15m interval data\n",
    "data_timezone = \"Europe/Malta\" # timezone used to convert the data timezone from source\n",
    "\n",
    "# currency pairs which will be investigated in this study\n",
    "# we will be investigating 6 different cryptocurrencies pairs \n",
    "# all of which are against USDT which is considered as a stable coin \n",
    "\n",
    "# Bitcoin/Tether    17-08-2017 to 01-06-2019\n",
    "# Ethereum/Tether   17-08-2017 to 01-06-2019\n",
    "# Zerps/Tether      04-05-2018 to 01-06-2019\n",
    "# Litcoin/Tether    13-12-2017 to 01-06-2019\n",
    "# EOS/Tether        28-05-2018 to 01-06-2019\n",
    "# Stellar/Tether    31-05-2018 to 01-06-2019\n",
    "currency_pairs = [(\"BTCUSDT\", \"17 Aug, 2017\", \"1 Jun, 2019\"), \n",
    "                  (\"ETHUSDT\", \"17 Aug, 2017\", \"1 Jun, 2019\"), \n",
    "                  (\"XRPUSDT\", \"4 May, 2018\",  \"1 Jun, 2019\"),  \n",
    "                  (\"LTCUSDT\", \"13 Dec, 2017\", \"1 Jun, 2019\"),\n",
    "                  (\"EOSUSDT\", \"28 May, 2018\", \"1 Jun, 2019\"), \n",
    "                  (\"XLMUSDT\", \"31 May, 2018\", \"1 Jun, 2019\")] \n",
    "\n",
    "# create an ordered dictionary to hold all the data\n",
    "pairs_dict = OrderedDict()\n",
    "\n",
    "# check if data exists as .csv, if not found in data path, download the 15m data from Binance API \n",
    "# a new module was created in the lucrum library to access API's such as Binance \n",
    "\n",
    "# loop in each pair \n",
    "for pair in currency_pairs:\n",
    "    \n",
    "    # get attributes from pair tuple \n",
    "    tmp_pair = pair[0]   # get pair\n",
    "    tmp_start = pair[1]  # get start date\n",
    "    tmp_end = pair[2]    # get end date \n",
    "    \n",
    "    # get path from the attributes in the tuple \n",
    "    tmp_path = \"{0}{1}{2}\".format(data_path, tmp_pair, data_format)\n",
    "    my_file = Path(tmp_path)\n",
    "    \n",
    "    # check if path exists \n",
    "    if my_file.is_file(): # file exists\n",
    "        print(\"{0} file existed, retrieving data from file.\".format(tmp_pair))\n",
    "    \n",
    "        # load data from path \n",
    "        price_history = pd.read_csv(tmp_path)\n",
    "                \n",
    "        # convert datetime types\n",
    "        price_history[\"open_time\"] = pd.to_datetime(price_history[\"open_time\"].str.split(\"+\").str[0])\n",
    "        price_history[\"close_time\"] = pd.to_datetime(price_history[\"close_time\"].str.split(\"+\").str[0])\n",
    "        \n",
    "        # add the dataframe to the dictionary\n",
    "        pairs_dict[tmp_pair] = price_history\n",
    "        print(price_history.dtypes)\n",
    "        \n",
    "    else: # file does not exist, download data from Binance API\n",
    "        print(\"{0} file does not exist, downloading from {1}.\".format(tmp_pair, data_source))\n",
    "        \n",
    "        # download from source provided, with the details provided\n",
    "        price_history = ldr.get_data(source=data_source,\n",
    "                                     symbols=tmp_pair,\n",
    "                                     start=tmp_start, \n",
    "                                     end=tmp_end,\n",
    "                                     interval=data_interval,\n",
    "                                     timezone=data_timezone)\n",
    "        \n",
    "        # save the dataframe as csv file to the path \n",
    "        price_history.to_csv(tmp_path, \n",
    "                             index=None, \n",
    "                             header=True)\n",
    "        \n",
    "        \n",
    "        # convert datetime types\n",
    "        price_history[\"open_time\"] = pd.to_datetime(price_history[\"open_time\"].str.split(\"+\").str[0])\n",
    "        price_history[\"close_time\"] = pd.to_datetime(price_history[\"close_time\"].str.split(\"+\").str[0])\n",
    "        \n",
    "        # add the dataframe to the dictionary\n",
    "        pairs_dict[tmp_pair] = price_history\n",
    "        print(price_history.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring Cryptocurrency Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exploring the following dataset: BTCUSDT\n",
      "\n",
      "Distribution Moments \n",
      "\tMean (1st): 0.000011\n",
      "\tSTD (2nd): 0.005573\n",
      "\tSkew (3rd): 0.045194\n",
      "\tKurtosis (4th): 21.390842\n",
      "\n",
      "Descriptive stats from pd describe\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    62315.000000\n",
       "mean         0.000011\n",
       "std          0.005573\n",
       "min         -0.078635\n",
       "25%         -0.001634\n",
       "50%          0.000022\n",
       "75%          0.001743\n",
       "max          0.090803\n",
       "Name: close, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------\n",
      "Exploring the following dataset: ETHUSDT\n",
      "\n",
      "Distribution Moments \n",
      "\tMean (1st): -0.000002\n",
      "\tSTD (2nd): 0.006652\n",
      "\tSkew (3rd): 0.009216\n",
      "\tKurtosis (4th): 25.606661\n",
      "\n",
      "Descriptive stats from pd describe\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    62315.000000\n",
       "mean        -0.000002\n",
       "std          0.006652\n",
       "min         -0.126609\n",
       "25%         -0.002244\n",
       "50%          0.000000\n",
       "75%          0.002315\n",
       "max          0.138168\n",
       "Name: close, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------\n",
      "Exploring the following dataset: XRPUSDT\n",
      "\n",
      "Distribution Moments \n",
      "\tMean (1st): -0.000020\n",
      "\tSTD (2nd): 0.005486\n",
      "\tSkew (3rd): 0.278156\n",
      "\tKurtosis (4th): 31.207233\n",
      "\n",
      "Descriptive stats from pd describe\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    37513.000000\n",
       "mean        -0.000020\n",
       "std          0.005486\n",
       "min         -0.106577\n",
       "25%         -0.001931\n",
       "50%         -0.000031\n",
       "75%          0.001889\n",
       "max          0.097354\n",
       "Name: close, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------\n",
      "Exploring the following dataset: LTCUSDT\n",
      "\n",
      "Distribution Moments \n",
      "\tMean (1st): -0.000018\n",
      "\tSTD (2nd): 0.006991\n",
      "\tSkew (3rd): 0.109748\n",
      "\tKurtosis (4th): 30.928835\n",
      "\n",
      "Descriptive stats from pd describe\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    51016.000000\n",
       "mean        -0.000018\n",
       "std          0.006991\n",
       "min         -0.152350\n",
       "25%         -0.002466\n",
       "50%          0.000000\n",
       "75%          0.002401\n",
       "max          0.136735\n",
       "Name: close, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------\n",
      "Exploring the following dataset: EOSUSDT\n",
      "\n",
      "Distribution Moments \n",
      "\tMean (1st): -0.000011\n",
      "\tSTD (2nd): 0.005942\n",
      "\tSkew (3rd): -0.623118\n",
      "\tKurtosis (4th): 32.572065\n",
      "\n",
      "Descriptive stats from pd describe\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    35221.000000\n",
       "mean        -0.000011\n",
       "std          0.005942\n",
       "min         -0.147268\n",
       "25%         -0.002175\n",
       "50%         -0.000014\n",
       "75%          0.002147\n",
       "max          0.080855\n",
       "Name: close, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------\n",
      "Exploring the following dataset: XLMUSDT\n",
      "\n",
      "Distribution Moments \n",
      "\tMean (1st): -0.000023\n",
      "\tSTD (2nd): 0.005855\n",
      "\tSkew (3rd): 0.125108\n",
      "\tKurtosis (4th): 16.640248\n",
      "\n",
      "Descriptive stats from pd describe\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    34915.000000\n",
       "mean        -0.000023\n",
       "std          0.005855\n",
       "min         -0.077377\n",
       "25%         -0.002338\n",
       "50%          0.000000\n",
       "75%          0.002278\n",
       "max          0.094950\n",
       "Name: close, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "##################################################################\n",
    "# Explore the underlying distributions for the log returns, \n",
    "# and show other descriptive statistics. \n",
    "##################################################################\n",
    "\n",
    "# decimal places \n",
    "ndigits = 6\n",
    "\n",
    "# loop in all the datasets \n",
    "for key, value in pairs_dict.items(): \n",
    "    \n",
    "    # explore dataset \n",
    "    print(\"Exploring the following dataset: {}\".format(key))\n",
    "    \n",
    "    # compute log returns\n",
    "    tmp_log_returns = np.log(pairs_dict[key].close / pairs_dict[key].close.shift(1))\n",
    "        \n",
    "    # drop nan from the shift 1 \n",
    "    tmp_log_returns.dropna(inplace=True)\n",
    "    \n",
    "    # show 4 moments of distribution \n",
    "    # - mean: 1st moment \n",
    "    # - standard deviation: 2nd moment \n",
    "    # - skew: 3rd moment \n",
    "    # - kurtosis: 4th moment \n",
    "    ds_mean, ds_std, ds_skew, ds_kurtosis = algo.dist_moments(tmp_log_returns)\n",
    "    \n",
    "    # print 4 distribution moments\n",
    "    print(\"\\nDistribution Moments \"+\n",
    "          \"\\n\\tMean (1st): {:.6f}\".format(round(ds_mean, ndigits)) +\n",
    "          \"\\n\\tSTD (2nd): {0}\".format(round(ds_std, ndigits)) +\n",
    "          \"\\n\\tSkew (3rd): {0}\".format(round(ds_skew, ndigits)) +\n",
    "          \"\\n\\tKurtosis (4th): {0}\".format(round(ds_kurtosis, ndigits)))\n",
    "    \n",
    "    # print other stats from pandas describe\n",
    "    print(\"\\nDescriptive stats from pd describe\")\n",
    "    display(tmp_log_returns.describe())\n",
    "    print(\"---------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Splitting Function for In Sample and Out of Sample\n",
    "\n",
    "The function below is used to split the dataset, given that the dataset is ordered by time.\n",
    "\n",
    "<br>\n",
    "We will apply the following splits (for all the datasets)\n",
    "\n",
    "1) split in-sample and out-sample (80:20)\n",
    "\n",
    "2) re-split the in-sample to train and validation (80:20) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##################################################################\n",
    "# Dataset splitting function for Train/Validation/Test Splits \n",
    "##################################################################\n",
    "\n",
    "# function to split ordered time series dataset\n",
    "def split_dataset(data, split_perc):\n",
    "    \"\"\"Splits ordered time series pandas dataframe.\n",
    "\n",
    "    Parameters \n",
    "    ----------\n",
    "    data: pandas dataframe \n",
    "        An ordered dataframe (ordered by time ASC).\n",
    "    split_perc: float\n",
    "        A float value which specified the percentage of the split, \n",
    "        must be between 0 and 1, where 0 and 1 are exclusive. \n",
    "        \n",
    "    Returns  \n",
    "    -------\n",
    "    pandas dataframe: \n",
    "        A dataframe split with a total of the split_perc (%) specified.\n",
    "    pandas dataframe: \n",
    "        Another dataframe with the remaining date (1 - split_perc (%)).\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # print date range for total samples before split\n",
    "    time_from = data.head(1)[\"open_time\"].astype(str).values[0]\n",
    "    time_to = data.tail(1)[\"close_time\"].astype(str).values[0]\n",
    "    print(\"[From {} to {}]\".format(time_from, time_to))  \n",
    "    \n",
    "    # print total samples before split \n",
    "    total_rows = data.shape[0]\n",
    "    print(\"Total samples before splitting: {}\".format(total_rows))\n",
    "     \n",
    "    # caclulcate the total number of rows given it will be \n",
    "    # split by the percentage specified \n",
    "    first_split_n = int(total_rows * split_perc)\n",
    "    \n",
    "    # get splits \n",
    "    first_split, second_split = data.iloc[0:first_split_n, :], data.iloc[first_split_n:total_rows, :]\n",
    "    \n",
    "    # percentage of split \n",
    "    first_perc = round(split_perc * 100, 2)\n",
    "    \n",
    "    # print stats for first split \n",
    "    time_from = first_split.head(1)[\"open_time\"].astype(str).values[0]\n",
    "    time_to = first_split.tail(1)[\"close_time\"].astype(str).values[0]\n",
    "    print(\"\\n[From {} to {} ({}%)]\".format(time_from, \n",
    "                                           time_to, \n",
    "                                           first_perc))  \n",
    "    \n",
    "    # print total samples for first split \n",
    "    print(\"Total samples in first split: {} ({}%)\".format(first_split.shape[0], first_perc))\n",
    "    \n",
    "    # percentage for second split \n",
    "    second_perc = 100.00 - first_perc\n",
    "    \n",
    "    # print stats for second split \n",
    "    time_from = second_split.head(1)[\"open_time\"].astype(str).values[0]\n",
    "    time_to = second_split.tail(1)[\"close_time\"].astype(str).values[0]\n",
    "    print(\"\\n[From {} to {} ({}%)]\".format(time_from, \n",
    "                                           time_to, \n",
    "                                           second_perc))  \n",
    "    \n",
    "    # print total samples for second split \n",
    "    print(\"Total samples in first split: {} ({}%)\".format(second_split.shape[0], second_perc))\n",
    "    \n",
    "    # returns both splits \n",
    "    return first_split, second_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPLIT INSAMPLE AND OUTSAMPLE DATASETS\n",
      "-----------------------------------------------------\n",
      "Splitting in/out samples for: BTCUSDT \n",
      "\n",
      "[From 2017-08-17 06:00:00 to 2019-06-01 02:14:59.999]\n",
      "Total samples before splitting: 62316\n",
      "\n",
      "[From 2017-08-17 06:00:00 to 2019-01-21 13:14:59.999 (80.0%)]\n",
      "Total samples in first split: 49852 (80.0%)\n",
      "\n",
      "[From 2019-01-21 13:15:00 to 2019-06-01 02:14:59.999 (20.0%)]\n",
      "Total samples in first split: 12464 (20.0%)\n",
      "-----------------------------------------------------\n",
      "\n",
      "-----------------------------------------------------\n",
      "Splitting in/out samples for: ETHUSDT \n",
      "\n",
      "[From 2017-08-17 06:00:00 to 2019-06-01 02:14:59.999]\n",
      "Total samples before splitting: 62316\n",
      "\n",
      "[From 2017-08-17 06:00:00 to 2019-01-21 13:14:59.999 (80.0%)]\n",
      "Total samples in first split: 49852 (80.0%)\n",
      "\n",
      "[From 2019-01-21 13:15:00 to 2019-06-01 02:14:59.999 (20.0%)]\n",
      "Total samples in first split: 12464 (20.0%)\n",
      "-----------------------------------------------------\n",
      "\n",
      "-----------------------------------------------------\n",
      "Splitting in/out samples for: XRPUSDT \n",
      "\n",
      "[From 2018-05-04 10:00:00 to 2019-06-01 02:14:59.999]\n",
      "Total samples before splitting: 37514\n",
      "\n",
      "[From 2018-05-04 10:00:00 to 2019-03-14 11:29:59.999 (80.0%)]\n",
      "Total samples in first split: 30011 (80.0%)\n",
      "\n",
      "[From 2019-03-14 11:30:00 to 2019-06-01 02:14:59.999 (20.0%)]\n",
      "Total samples in first split: 7503 (20.0%)\n",
      "-----------------------------------------------------\n",
      "\n",
      "-----------------------------------------------------\n",
      "Splitting in/out samples for: LTCUSDT \n",
      "\n",
      "[From 2017-12-13 04:30:00 to 2019-06-01 02:14:59.999]\n",
      "Total samples before splitting: 51017\n",
      "\n",
      "[From 2017-12-13 04:30:00 to 2019-02-14 02:14:59.999 (80.0%)]\n",
      "Total samples in first split: 40813 (80.0%)\n",
      "\n",
      "[From 2019-02-14 02:15:00 to 2019-06-01 02:14:59.999 (20.0%)]\n",
      "Total samples in first split: 10204 (20.0%)\n",
      "-----------------------------------------------------\n",
      "\n",
      "-----------------------------------------------------\n",
      "Splitting in/out samples for: EOSUSDT \n",
      "\n",
      "[From 2018-05-28 07:00:00 to 2019-06-01 02:14:59.999]\n",
      "Total samples before splitting: 35222\n",
      "\n",
      "[From 2018-05-28 07:00:00 to 2019-03-19 05:59:59.999 (80.0%)]\n",
      "Total samples in first split: 28177 (80.0%)\n",
      "\n",
      "[From 2019-03-19 06:00:00 to 2019-06-01 02:14:59.999 (20.0%)]\n",
      "Total samples in first split: 7045 (20.0%)\n",
      "-----------------------------------------------------\n",
      "\n",
      "-----------------------------------------------------\n",
      "Splitting in/out samples for: XLMUSDT \n",
      "\n",
      "[From 2018-05-31 11:30:00 to 2019-06-01 02:14:59.999]\n",
      "Total samples before splitting: 34916\n",
      "\n",
      "[From 2018-05-31 11:30:00 to 2019-03-19 21:14:59.999 (80.0%)]\n",
      "Total samples in first split: 27932 (80.0%)\n",
      "\n",
      "[From 2019-03-19 21:15:00 to 2019-06-01 02:14:59.999 (20.0%)]\n",
      "Total samples in first split: 6984 (20.0%)\n",
      "-----------------------------------------------------\n",
      "\n",
      "SPLIT TRAINING AND VALIDATION DATASETS\n",
      "-----------------------------------------------------\n",
      "Splitting train/validation samples for: BTCUSDT \n",
      "\n",
      "[From 2017-08-17 06:00:00 to 2019-01-21 13:14:59.999]\n",
      "Total samples before splitting: 49852\n",
      "\n",
      "[From 2017-08-17 06:00:00 to 2018-10-09 06:59:59.999 (80.0%)]\n",
      "Total samples in first split: 39881 (80.0%)\n",
      "\n",
      "[From 2018-10-09 07:00:00 to 2019-01-21 13:14:59.999 (20.0%)]\n",
      "Total samples in first split: 9971 (20.0%)\n",
      "-----------------------------------------------------\n",
      "\n",
      "-----------------------------------------------------\n",
      "Splitting train/validation samples for: ETHUSDT \n",
      "\n",
      "[From 2017-08-17 06:00:00 to 2019-01-21 13:14:59.999]\n",
      "Total samples before splitting: 49852\n",
      "\n",
      "[From 2017-08-17 06:00:00 to 2018-10-09 06:59:59.999 (80.0%)]\n",
      "Total samples in first split: 39881 (80.0%)\n",
      "\n",
      "[From 2018-10-09 07:00:00 to 2019-01-21 13:14:59.999 (20.0%)]\n",
      "Total samples in first split: 9971 (20.0%)\n",
      "-----------------------------------------------------\n",
      "\n",
      "-----------------------------------------------------\n",
      "Splitting train/validation samples for: XRPUSDT \n",
      "\n",
      "[From 2018-05-04 10:00:00 to 2019-03-14 11:29:59.999]\n",
      "Total samples before splitting: 30011\n",
      "\n",
      "[From 2018-05-04 10:00:00 to 2019-01-10 16:44:59.999 (80.0%)]\n",
      "Total samples in first split: 24008 (80.0%)\n",
      "\n",
      "[From 2019-01-10 16:45:00 to 2019-03-14 11:29:59.999 (20.0%)]\n",
      "Total samples in first split: 6003 (20.0%)\n",
      "-----------------------------------------------------\n",
      "\n",
      "-----------------------------------------------------\n",
      "Splitting train/validation samples for: LTCUSDT \n",
      "\n",
      "[From 2017-12-13 04:30:00 to 2019-02-14 02:14:59.999]\n",
      "Total samples before splitting: 40813\n",
      "\n",
      "[From 2017-12-13 04:30:00 to 2018-11-21 01:29:59.999 (80.0%)]\n",
      "Total samples in first split: 32650 (80.0%)\n",
      "\n",
      "[From 2018-11-21 01:30:00 to 2019-02-14 02:14:59.999 (20.0%)]\n",
      "Total samples in first split: 8163 (20.0%)\n",
      "-----------------------------------------------------\n",
      "\n",
      "-----------------------------------------------------\n",
      "Splitting train/validation samples for: EOSUSDT \n",
      "\n",
      "[From 2018-05-28 07:00:00 to 2019-03-19 05:59:59.999]\n",
      "Total samples before splitting: 28177\n",
      "\n",
      "[From 2018-05-28 07:00:00 to 2019-01-19 06:59:59.999 (80.0%)]\n",
      "Total samples in first split: 22541 (80.0%)\n",
      "\n",
      "[From 2019-01-19 07:00:00 to 2019-03-19 05:59:59.999 (20.0%)]\n",
      "Total samples in first split: 5636 (20.0%)\n",
      "-----------------------------------------------------\n",
      "\n",
      "-----------------------------------------------------\n",
      "Splitting train/validation samples for: XLMUSDT \n",
      "\n",
      "[From 2018-05-31 11:30:00 to 2019-03-19 21:14:59.999]\n",
      "Total samples before splitting: 27932\n",
      "\n",
      "[From 2018-05-31 11:30:00 to 2019-01-20 10:29:59.999 (80.0%)]\n",
      "Total samples in first split: 22345 (80.0%)\n",
      "\n",
      "[From 2019-01-20 10:30:00 to 2019-03-19 21:14:59.999 (20.0%)]\n",
      "Total samples in first split: 5587 (20.0%)\n",
      "-----------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##################################################################\n",
    "# Split the datasets for Train/Validation/Test Splits \n",
    "##################################################################\n",
    "\n",
    "# we will apply the following splits (for all the datasets)\n",
    "# 1) split in-sample and out-sample (80:20)\n",
    "# 2) re-split the in-sample to train and validation (80:20) \n",
    "\n",
    "split_percentage = 0.80        # for both splits we used 80:20\n",
    "insample_data = OrderedDict()  # create an ordered dictionary to hold insample\n",
    "outsample_data = OrderedDict() # create an ordered dictionary to hold insample\n",
    "\n",
    "# 1) split in-sample and out-sample (80:20)\n",
    "print(\"SPLIT INSAMPLE AND OUTSAMPLE DATASETS\")\n",
    "for key, value in pairs_dict.items(): # loop to split all the dataset to in/out samples\n",
    "    print(\"-----------------------------------------------------\")\n",
    "    print(\"Splitting in/out samples for: {} \\n\".format(key))\n",
    "    \n",
    "    # get splits \n",
    "    insample_split, outsample_split = split_dataset(pairs_dict[key], split_percentage)\n",
    "    insample_data[key] = insample_split    # add insample split to insample dictionary \n",
    "    outsample_data[key] = outsample_split  # add outsample split to outsample dictionary \n",
    "    print(\"-----------------------------------------------------\\n\")\n",
    "    \n",
    "train_data = OrderedDict()      # create an ordered dictionary to hold training set\n",
    "validation_data = OrderedDict() # create an ordered dictionary to hold validation set \n",
    "\n",
    "# 2) re-split the in-sample to train and validation (80:20) \n",
    "print(\"SPLIT TRAINING AND VALIDATION DATASETS\")\n",
    "for key, value in insample_data.items(): # loop to split all the dataset to train/validation\n",
    "    print(\"-----------------------------------------------------\")\n",
    "    print(\"Splitting train/validation samples for: {} \\n\".format(key))\n",
    "    \n",
    "    # get splits\n",
    "    training_split, validation_split = split_dataset(insample_data[key], split_percentage)\n",
    "    train_data[key] = train_data            # add training split to training dictionary \n",
    "    validation_data[key] = validation_split # add validation split to validation dictionary\n",
    "    print(\"-----------------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Buy and Hold Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Technical Indicators Strategies "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to test out 4 different types of technical indicators. The following are the types of indicators we will be testing: \n",
    "\n",
    "1) <b>Trend Indicators</b> \n",
    "2) <b>Momentum Indicators</b>\n",
    "3) <b>Volatility Indicators</b>\n",
    "4) <b>Volume Indicators</b>\n",
    "\n",
    "<i>NOTE: Sometimes trend and momentum indicators are use interchangeably, meaning some may refer to trend indicators as momentum indicators and some may refer to momentum indicators as trend indicators.</i>\n",
    "\n",
    "<i>NOTE: Sometimes volatility and volume indicators are use interchangeably, meaning some may refer to volatility indicators as volume indicators and some may refer to volume indicators as volatility indicators.</i>\n",
    "\n",
    "For each type we have chosen to investigate the following technical indicators:\n",
    "\n",
    "1) <b>Trend Indicator</b>\n",
    "- Short-Window EMA: Exponential Moving Average\n",
    "- Long-Window EMA: Exponential Moving Average\n",
    "\n",
    "2) <b>Momentum Indicator</b>\n",
    "- RSI: Relative Strength Index\n",
    "\n",
    "3) <b>Volatility Indicator</b>\n",
    "- ATR: Average True Range\n",
    "\n",
    "4) <b>Volume Indicator</b>\n",
    "- OBV: On Balance Volume\n",
    "\n",
    "So in total we will make use of 6 technical indicators. Now we will test these standard technical indicators strategies, each indicator with it’s own strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exponential Moving Average (Crossover) Strategy \n",
    "\n",
    "This strategy which will be applied is one of the most simplest strategies used when using an Exponential Moving Average (EMA) indicator or any other moving average indicator. We take the following positions based on the following rules:\n",
    "\n",
    "<br>\n",
    "<i>\n",
    "If EMA_shortwindow > EMA_longwindow <br>\n",
    "&ensp;then OpenLongPosition <br>\n",
    "else <br>\n",
    "&ensp;OpenShortPosition \n",
    "</i>\n",
    "\n",
    "\n",
    "This strategy is usually referred to as the moving average crossover strategy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open_time</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>close_time</th>\n",
       "      <th>trades</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-08-17 06:00:00</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>4280.56</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>2017-08-17 06:14:59.999</td>\n",
       "      <td>9</td>\n",
       "      <td>2.189061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-08-17 06:15:00</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>4270.41</td>\n",
       "      <td>4261.32</td>\n",
       "      <td>4261.45</td>\n",
       "      <td>2017-08-17 06:29:59.999</td>\n",
       "      <td>40</td>\n",
       "      <td>9.119865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-08-17 06:30:00</td>\n",
       "      <td>4280.00</td>\n",
       "      <td>4310.07</td>\n",
       "      <td>4267.99</td>\n",
       "      <td>4310.07</td>\n",
       "      <td>2017-08-17 06:44:59.999</td>\n",
       "      <td>58</td>\n",
       "      <td>21.923552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-08-17 06:45:00</td>\n",
       "      <td>4310.07</td>\n",
       "      <td>4313.62</td>\n",
       "      <td>4291.37</td>\n",
       "      <td>4308.83</td>\n",
       "      <td>2017-08-17 06:59:59.999</td>\n",
       "      <td>64</td>\n",
       "      <td>13.948531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-08-17 07:00:00</td>\n",
       "      <td>4308.83</td>\n",
       "      <td>4328.69</td>\n",
       "      <td>4304.31</td>\n",
       "      <td>4304.31</td>\n",
       "      <td>2017-08-17 07:14:59.999</td>\n",
       "      <td>44</td>\n",
       "      <td>5.101153</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            open_time     open     high      low    close  \\\n",
       "0 2017-08-17 06:00:00  4261.48  4280.56  4261.48  4261.48   \n",
       "1 2017-08-17 06:15:00  4261.48  4270.41  4261.32  4261.45   \n",
       "2 2017-08-17 06:30:00  4280.00  4310.07  4267.99  4310.07   \n",
       "3 2017-08-17 06:45:00  4310.07  4313.62  4291.37  4308.83   \n",
       "4 2017-08-17 07:00:00  4308.83  4328.69  4304.31  4304.31   \n",
       "\n",
       "               close_time  trades     volume  \n",
       "0 2017-08-17 06:14:59.999       9   2.189061  \n",
       "1 2017-08-17 06:29:59.999      40   9.119865  \n",
       "2 2017-08-17 06:44:59.999      58  21.923552  \n",
       "3 2017-08-17 06:59:59.999      64  13.948531  \n",
       "4 2017-08-17 07:14:59.999      44   5.101153  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(pairs_dict[\"BTCUSDT\"].head())\n",
    "# ##################################################################\n",
    "# # Exponential Moving Average Crossover Strategy (BASELINE MODEL)\n",
    "# ##################################################################\n",
    "\n",
    "# # we only show the first n and last n when plotting the positions \n",
    "# # as the there are many points since we are using the 15m interval\n",
    "# position_plot_n = 500\n",
    "\n",
    "# # create a new instance for moving average strategy \n",
    "# macrossover = MACrossoverAlgo()\n",
    "\n",
    "# # tune ema crossover method ('brute force method')\n",
    "# # for all the pairs found in the dataset \n",
    "# for key, value in pairs_dict.items():\n",
    "        \n",
    "#     # print the pair which is being tuned \n",
    "#     print(\"Tuning EMA strategy using brute force for {}\".format(key))\n",
    "\n",
    "#     # make copy of dataframe \n",
    "#     current_pair = pairs_dict[key].copy()\n",
    "    \n",
    "#     # setup initial values \n",
    "#     best_lead = 0\n",
    "#     best_lag = 0\n",
    "#     best_pl = -float(\"inf\") # set initial profit/loss to inf \n",
    "    \n",
    "#     # loop to try different windows for both lead and lag windows\n",
    "#     for i in range(5, 50, 5):     # lead window (steps of 5)\n",
    "#         for j in range(20, 80, 5): # lag window  (steps of 5)\n",
    "\n",
    "#             # lead window must be shorter than the lag window \n",
    "#             if j > i:\n",
    "                \n",
    "#                 # note there are also some traders which use EMA for lead and SMA for lag window \n",
    "#                 # this MACrossoverAlgo can be used to test this strategy too, as it accepts the \n",
    "#                 # type of moving average to use 'ema' or 'sma', in our case we will use 'ema' for both\n",
    "#                 macrossover.gen_features(data=current_pair,\n",
    "#                                          lead=\"sma\", # we will use EMA moving average type for lead \n",
    "#                                          lead_t=i,   # set window for short/lead \n",
    "#                                          lag=\"sma\",  # we will use EMA moving average type for lead \n",
    "#                                          lag_t=j)    # set window for long/lag\n",
    "\n",
    "#                 # generates positions based on crossover \n",
    "#                 # a value of '1' indicates a long position, while a value of '-1' indicates a short position \n",
    "#                 macrossover.gen_positions(data=current_pair)\n",
    "\n",
    "#                 # evaluate positions by calculating profit and loss \n",
    "#                 macrossover.evaluate(data=current_pair)\n",
    "\n",
    "#                 # check if we found a new best \n",
    "#                 if current_pair.cum_pl.iloc[-1] > best_pl:\n",
    "#                     best_lead = i\n",
    "#                     best_lag = j\n",
    "#                     best_pl = current_pair.cum_pl.iloc[-1]\n",
    "\n",
    "#     # show plots for best  \n",
    "#     macrossover.gen_features(data=current_pair,\n",
    "#                              lead=\"ema\",         # we will use EMA moving average type for lead \n",
    "#                              lead_t=best_lead,   # set window for short/lead \n",
    "#                              lag=\"ema\",          # we will use EMA moving average type for lead \n",
    "#                              lag_t=best_lag)     # set window for long/\n",
    "    \n",
    "#     display(current_pair.tail())\n",
    "    \n",
    "#     # print statistics for strategy \n",
    "#     macrossover.gen_positions(data=current_pair)\n",
    "#     macrossover.evaluate(data=current_pair) \n",
    "\n",
    "#     # print best windows \n",
    "#     print(\"[BEST] Lead({}) / Lag({})\\n\".format(best_lead, best_lag))\n",
    "    \n",
    "#     # print performance\n",
    "#     macrossover.stats_perf(data=current_pair)\n",
    "    \n",
    "#     # set open time as index for plots\n",
    "#     current_pair.set_index(\"open_time\", inplace=True)\n",
    "#     # for the position plots we only show the first and last 500\n",
    "#     print(\"\\nFirst 500 poistions taken for: {}\".format(key))\n",
    "#     macrossover.plot_pos(data=current_pair.head(position_plot_n))\n",
    "#     macrossover.plot_perf(data=current_pair.head(position_plot_n))\n",
    "\n",
    "#     print(\"Last 500 poistions taken for: {}\".format(key))\n",
    "#     macrossover.plot_pos(data=current_pair.tail(position_plot_n))\n",
    "#     macrossover.plot_perf(data=current_pair.tail(position_plot_n))\n",
    "\n",
    "#     # plot performance for the whole range\n",
    "#     print(\"Equity Curve for the whole date range for: {}\".format(key))\n",
    "#     macrossover.plot_perf(data=current_pair)\n",
    "    \n",
    "#     print(\"-------------------------------END--------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# display(pairs_dict[\"BTCUSDT\"].head())\n",
    "# display(pairs_dict[\"BTCUSDT\"].tail())\n",
    "\n",
    "\n",
    "# from pandas_datareader import data as pdr\n",
    "# import yfinance as yf\n",
    "# yf.pdr_override()\n",
    "# macrossover = MACrossoverAlgo()\n",
    "\n",
    "# display()\n",
    "\n",
    "# #test this shit out mate \n",
    "# sym1 = \"MSFT\"\n",
    "# data = pdr.get_data_yahoo(\"MSFT\", start=\"2016-01-01\", end=\"2017-01-01\")\n",
    "# data[\"price\"] = data[\"Adj Close\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# #display(data)\n",
    "\n",
    "# print(data.iloc[0].index)\n",
    "\n",
    "# data[\"close\"] = data[\"Adj Close\"]\n",
    "# data[\"open\"] = data[\"Open\"]\n",
    "# data[\"high\"] = data[\"High\"]\n",
    "# data[\"low\"] = data[\"Low\"]\n",
    "# data[\"volume\"] = data[\"Volume\"]\n",
    "\n",
    "# data[\"open_time\"] = \"test\"\n",
    "\n",
    "# data[\"close_time\"] = \"test\"\n",
    "\n",
    "# macrossover.gen_features(data=data,\n",
    "#                         lead=\"sma\", # we will use EMA moving average type for lead \n",
    "#                         lead_t=5,   # set window for short/lead \n",
    "#                         lag=\"sma\",  # we will use EMA moving average type for lead \n",
    "#                         lag_t=30)    # set window for long/lag\n",
    "\n",
    "# macrossover.gen_positions(data=data)\n",
    "# macrossover.plot_pos(data=data)\n",
    "# macrossover.evaluate(data=data)\n",
    "# macrossover.plot_perf(data=data)\n",
    "# macrossover.stats_perf(data=data)\n",
    "\n",
    "# display(data.tail())\n",
    "# print(round(data.iloc[-1].cum_pl * 100, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# display(buy_n_hold_test.head())\n",
    "# display(buy_n_hold_test.tail())\n",
    "\n",
    "# buy_n_hold_test = pairs_dict[\"BTCUSDT\"].copy()\n",
    "# buy_n_hold_test[\"logprices\"] = np.log(buy_n_hold_test[\"close\"])\n",
    "# buy_n_hold_test[\"log_returns\"] = np.log(buy_n_hold_test.close / buy_n_hold_test.close.shift(1))\n",
    "# buy_n_hold_test[\"cum_pl\"] = buy_n_hold_test[\"log_returns\"].cumsum()\n",
    "\n",
    "# round(np.exp(buy_n_hold_test.iloc[-1].cum_pl) * 100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# for key, value in pairs_dict.items():\n",
    "#     print(key)\n",
    "#     current_pair = pairs_dict[key].copy()\n",
    "#     display(current_pair.head(1))\n",
    "#     display(current_pair.tail(1))\n",
    "    \n",
    "#     current_pair[\"logprices\"] = np.log(current_pair[\"close\"])\n",
    "#     current_pair[\"log_returns\"] = np.log(current_pair.close / current_pair.close.shift(1))\n",
    "#     current_pair[\"cum_pl\"] = current_pair[\"log_returns\"].cumsum()\n",
    "\n",
    "#     r = (np.exp(current_pair.iloc[-1].cum_pl) - 1) * 100\n",
    "#     print(round(r, 2))\n",
    "#     print(\"Log Retun Percentage Buy n hold: {}%\".format(round(current_pair.iloc[-1].cum_pl * 100, 2)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ari5123] *",
   "language": "python",
   "name": "conda-env-ari5123-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
